{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "y_objective_data = [2.5,3,1.6,8,4.56,5.25,7,5.25,6.5,10.5,25,2.3,5.4,6.8,7.9,4.5,3.5,3,2,1,0,5.6,7,8.5,9,10.2,5,4.3,2,4.3]\n",
    "print(len(y_objective_data))\n",
    "objective_data = np.array(y_objective_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "predictions_m1 = [3,2.9,2,8.1,4,5,7.8,6,6,10,10,2,5,7,8.5,4,4,4.5,2.5,1.23,1,5,6.8,9.6,10.2,10,4.9,4,0,4]\n",
    "print(len(predictions_m1))\n",
    "predictions_m1 = np.array(predictions_m1)\n",
    "predictions_m2 = [2,2,2,7,5,5,8,5,7,11,24,2,6,8,8.5,5,5,4.5,3,0.9,0.5,6,7.3,9,10,10,5.2,4.6,1.9,5]\n",
    "print(len(predictions_m2))\n",
    "prections_m2 = np.array(predictions_m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas para Modelo 1:\n",
      "MAE: 1.0396666666666667\n",
      "MSE: 7.9830499999999995\n",
      "RMSE: 2.8254291709402306\n",
      "R^2: 0.5991824683452813\n",
      "\n",
      "Métricas para Modelo 2:\n",
      "MAE: 0.6113333333333333\n",
      "MSE: 0.5219533333333334\n",
      "RMSE: 0.7224633785413164\n",
      "R^2: 0.973793469074399\n",
      "\n",
      "Modelo 2 es mejor en términos de RMSE.\n",
      "Modelo 2 es mejor en términos de MAE.\n",
      "Modelo 2 es mejor en términos de R^2.\n"
     ]
    }
   ],
   "source": [
    "#Se haran las metricas con formulas manualmente y con sklearn para comparar los resultados y ver si son iguales\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Calcular métricas para Modelo 1\n",
    "mae_m1 = mean_absolute_error(y_objective_data, predictions_m1)\n",
    "mse_m1 = mean_squared_error(y_objective_data, predictions_m1)\n",
    "rmse_m1 = np.sqrt(mse_m1)\n",
    "r2_m1 = r2_score(y_objective_data, predictions_m1)\n",
    "\n",
    "# Calcular métricas para Modelo 2\n",
    "mae_m2 = mean_absolute_error(y_objective_data, predictions_m2)\n",
    "mse_m2 = mean_squared_error(y_objective_data, predictions_m2)\n",
    "rmse_m2 = np.sqrt(mse_m2)\n",
    "r2_m2 = r2_score(y_objective_data, predictions_m2)\n",
    "\n",
    "# Imprimir las métricas para ambos modelos\n",
    "print(\"Métricas para Modelo 1:\")\n",
    "print(\"MAE:\", mae_m1)\n",
    "print(\"MSE:\", mse_m1)\n",
    "print(\"RMSE:\", rmse_m1)\n",
    "print(\"R^2:\", r2_m1)\n",
    "print()\n",
    "\n",
    "print(\"Métricas para Modelo 2:\")\n",
    "print(\"MAE:\", mae_m2)\n",
    "print(\"MSE:\", mse_m2)\n",
    "print(\"RMSE:\", rmse_m2)\n",
    "print(\"R^2:\", r2_m2)\n",
    "print()\n",
    "\n",
    "# Comparar los modelos basados en las métricas\n",
    "# Comparar los modelos basados en las métricas\n",
    "if rmse_m1 < rmse_m2:\n",
    "    print(\"Modelo 1 es mejor en términos de RMSE.\")\n",
    "elif rmse_m2 < rmse_m1:\n",
    "    print(\"Modelo 2 es mejor en términos de RMSE.\")\n",
    "else:\n",
    "    print(\"Ambos modelos son comparables en términos de RMSE.\")\n",
    "\n",
    "if mae_m1 < mae_m2:\n",
    "    print(\"Modelo 1 es mejor en términos de MAE.\")\n",
    "elif mae_m2 < mae_m1:\n",
    "    print(\"Modelo 2 es mejor en términos de MAE.\")\n",
    "else:\n",
    "    print(\"Ambos modelos son comparables en términos de MAE.\")\n",
    "\n",
    "if r2_m1 > r2_m2:\n",
    "    print(\"Modelo 1 es mejor en términos de R^2.\")\n",
    "elif r2_m2 > r2_m1:\n",
    "    print(\"Modelo 2 es mejor en términos de R^2.\")\n",
    "else:\n",
    "    print(\"Ambos modelos son comparables en términos de R^2.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE_M1 1.0396666666666665 MAE_M2 0.6113333333333333\n"
     ]
    }
   ],
   "source": [
    "#Mean Absolute Error (MAE):\n",
    "n = len(y_objective_data)\n",
    "mae_m1 = sum(abs(y - y_hat) for y, y_hat in zip(y_objective_data, predictions_m1)) / n\n",
    "mae_m2 = sum(abs(y - y_hat) for y, y_hat in zip(y_objective_data, predictions_m2)) / n\n",
    "\n",
    "print('MAE_M1',mae_m1, 'MAE_M2',mae_m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE_1 7.98305 MSE_2 0.5219533333333334\n"
     ]
    }
   ],
   "source": [
    "#Mean Squared Error (MSE):\n",
    "mse_m1 = sum((y - y_hat)**2 for y, y_hat in zip(y_objective_data, predictions_m1)) / n\n",
    "mse_m2 = sum((y - y_hat)**2 for y, y_hat in zip(y_objective_data, predictions_m2)) / n\n",
    "\n",
    "print('MSE_1',mse_m1, 'MSE_2',mse_m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Model 1:  2.825429170940231\n",
      "RMSE for Model 2:  0.7224633785413164\n"
     ]
    }
   ],
   "source": [
    "#. Root Mean Squared Error (RMSE):\n",
    "rmse_m1 = mse_m1 ** 0.5\n",
    "rmse_m2 = mse_m2 ** 0.5\n",
    "\n",
    "print(\"RMSE for Model 1: \", rmse_m1)\n",
    "print(\"RMSE for Model 2: \", rmse_m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² for model 1:  0.5991824683452813\n",
      "R² for model 2:  0.973793469074399\n"
     ]
    }
   ],
   "source": [
    "#Coefficient of Determination (R²):\n",
    "mean_actual = sum(y_objective_data) / n\n",
    "ss_total = sum((y - mean_actual)**2 for y in y_objective_data)\n",
    "ss_residual_m1 = sum((y - y_hat)**2 for y, y_hat in zip(y_objective_data, predictions_m1))\n",
    "ss_residual_m2 = sum((y - y_hat)**2 for y, y_hat in zip(y_objective_data, predictions_m2))\n",
    "r2_m1 = 1 - ss_residual_m1 / ss_total\n",
    "r2_m2 = 1 - ss_residual_m2 / ss_total\n",
    "\n",
    "print(\"R² for model 1: \", r2_m1)\n",
    "print(\"R² for model 2: \", r2_m2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo 2 es mejor en términos de RMSE.\n",
      "Modelo 2 es mejor en términos de MAE.\n",
      "Modelo 2 es mejor en términos de R^2.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Comparar los modelos basados en las métricas\n",
    "if rmse_m1 < rmse_m2:\n",
    "    print(\"Modelo 1 es mejor en términos de RMSE.\")\n",
    "elif rmse_m2 < rmse_m1:\n",
    "    print(\"Modelo 2 es mejor en términos de RMSE.\")\n",
    "else:\n",
    "    print(\"Ambos modelos son comparables en términos de RMSE.\")\n",
    "\n",
    "if mae_m1 < mae_m2:\n",
    "    print(\"Modelo 1 es mejor en términos de MAE.\")\n",
    "elif mae_m2 < mae_m1:\n",
    "    print(\"Modelo 2 es mejor en términos de MAE.\")\n",
    "else:\n",
    "    print(\"Ambos modelos son comparables en términos de MAE.\")\n",
    "\n",
    "if r2_m1 > r2_m2:\n",
    "    print(\"Modelo 1 es mejor en términos de R^2.\")\n",
    "elif r2_m2 > r2_m1:\n",
    "    print(\"Modelo 2 es mejor en términos de R^2.\")\n",
    "else:\n",
    "    print(\"Ambos modelos son comparables en términos de R^2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Dado un problema de clasificación binaria (sólo existen dos clases), tenemos la salida de cada uno de 30 patrones en test para dos modelos distintos.\n",
    "a. Montar la matriz de confusión de cada modelo.\n",
    "b. Calcular las métricas de clasificación, excepto el AUC.\n",
    "c. Determinar qué modelo es mejor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "\n",
    "objective_class = [0,0,0,1,1,1,0,0,1,1,0,1,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "print(len(objective_class))\n",
    "objective_class = np.array(objective_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "predictions_m1 = [1,0,0,1,1,1,0,0,0,0,0,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "print(len(predictions_m1))\n",
    "predictions_m1 = np.array(predictions_m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "predictions_m2 = [0,0,0,1,1,1,0,1,1,1,1,0,1,1,1,1,1,0,0,0,0,1,1,0,0,0,1,0,1,1]\n",
    "print(len(predictions_m2))\n",
    "predictions_m2 = np.array(predictions_m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusión para Modelo 1:\n",
      "[[19  1]\n",
      " [ 4  6]]\n",
      "Matriz de Confusión para Modelo 2:\n",
      "[[12  8]\n",
      " [ 1  9]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "# Crear la matriz de confusión para cada modelo\n",
    "confusion_matrix_m1 = confusion_matrix(objective_class, predictions_m1)\n",
    "confusion_matrix_m2 = confusion_matrix(objective_class, predictions_m2)\n",
    "\n",
    "print(\"Matriz de Confusión para Modelo 1:\")\n",
    "print(confusion_matrix_m1)\n",
    "\n",
    "print(\"Matriz de Confusión para Modelo 2:\")\n",
    "print(confusion_matrix_m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metricas del modelo 1 \n",
      "Accuracy:  0.6666666666666666 \n",
      "Precision:  0.8260869565217391 \n",
      "Recall:  0.76 \n",
      "Specificity:  0.2 \n",
      "F-Score:  0.7916666666666667\n",
      "\n",
      "Metricas del modelo 2 \n",
      "Accuracy:  0.6666666666666666 \n",
      "Precision:  0.9230769230769231 \n",
      "Recall:  0.5714285714285714 \n",
      "Specificity:  0.8888888888888888 \n",
      "F-Score:  0.7058823529411765\n"
     ]
    }
   ],
   "source": [
    "#Calcular metricas de clasificacion\n",
    "\n",
    "\n",
    "def calculate_metrics(conf_matrix):\n",
    "    vp, vn, fp, fn = conf_matrix.ravel()\n",
    "    accuracy = (vp + vn) / (vp + vn + fp + fn)\n",
    "    precision = vp / (vp + fp)\n",
    "    recall = vp / (vp + fn)\n",
    "    specificity = vn / (vn + fp)\n",
    "    f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return accuracy, precision, recall, specificity, f_score\n",
    "\n",
    "accuracy_m1, precision_m1, recall_m1, specificity_m1, f_score_m1 = calculate_metrics(confusion_matrix_m1)\n",
    "accuracy_m2, precision_m2, recall_m2, specificity_m2, f_score_m2 = calculate_metrics(confusion_matrix_m2)\n",
    "\n",
    "print(\"Metricas del modelo 1\" , \"\\nAccuracy: \", accuracy_m1, \"\\nPrecision: \", precision_m1, \"\\nRecall: \", recall_m1, \"\\nSpecificity: \", specificity_m1, \"\\nF-Score: \", f_score_m1)\n",
    "\n",
    "print(\"\\nMetricas del modelo 2\" , \"\\nAccuracy: \", accuracy_m2, \"\\nPrecision: \", precision_m2, \"\\nRecall: \", recall_m2, \"\\nSpecificity: \", specificity_m2, \"\\nF-Score: \", f_score_m2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambos modelos son comparables en términos de Accuracy.\n",
      "Modelo 2 es mejor en términos de Precision.\n",
      "Modelo 1 es mejor en términos de Recall.\n",
      "Modelo 2 es mejor en términos de Specificity.\n",
      "Modelo 1 es mejor en términos de F-Score.\n"
     ]
    }
   ],
   "source": [
    "# Comparar los modelos basados en las métricas\n",
    "\n",
    "# Comparar los modelos basados en cada métrica individual\n",
    "if accuracy_m1 > accuracy_m2:\n",
    "    print(\"Modelo 1 es mejor en términos de Accuracy.\")\n",
    "elif accuracy_m2 > accuracy_m1:\n",
    "    print(\"Modelo 2 es mejor en términos de Accuracy.\")\n",
    "else:\n",
    "    print(\"Ambos modelos son comparables en términos de Accuracy.\")\n",
    "\n",
    "if precision_m1 > precision_m2:\n",
    "    print(\"Modelo 1 es mejor en términos de Precision.\")\n",
    "elif precision_m2 > precision_m1:\n",
    "    print(\"Modelo 2 es mejor en términos de Precision.\")\n",
    "else:\n",
    "    print(\"Ambos modelos son comparables en términos de Precision.\")\n",
    "\n",
    "if recall_m1 > recall_m2:\n",
    "    print(\"Modelo 1 es mejor en términos de Recall.\")\n",
    "elif recall_m2 > recall_m1:\n",
    "    print(\"Modelo 2 es mejor en términos de Recall.\")\n",
    "else:\n",
    "    print(\"Ambos modelos son comparables en términos de Recall.\")\n",
    "\n",
    "if specificity_m1 > specificity_m2:\n",
    "    print(\"Modelo 1 es mejor en términos de Specificity.\")\n",
    "elif specificity_m2 > specificity_m1:\n",
    "    print(\"Modelo 2 es mejor en términos de Specificity.\")\n",
    "else:\n",
    "    print(\"Ambos modelos son comparables en términos de Specificity.\")\n",
    "\n",
    "if f_score_m1 > f_score_m2:\n",
    "    print(\"Modelo 1 es mejor en términos de F-Score.\")\n",
    "elif f_score_m2 > f_score_m1:\n",
    "    print(\"Modelo 2 es mejor en términos de F-Score.\")\n",
    "else:\n",
    "    print(\"Ambos modelos son comparables en términos de F-Score.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo 2 es mejor en términos de métricas ponderadas.\n"
     ]
    }
   ],
   "source": [
    "def calculate_weighted_average_metrics(metrics):\n",
    "    # Pesos para las métricas, puedes ajustarlos según la importancia que desees dar a cada métrica\n",
    "    weights = {\n",
    "        'accuracy': 1,\n",
    "        'precision': 1,\n",
    "        'recall': 1,\n",
    "        'specificity': 1,\n",
    "        'f_score': 1\n",
    "    }\n",
    "    \n",
    "    weighted_sum = sum(metrics[metric] * weights[metric] for metric in metrics)\n",
    "    total_weight = sum(weights.values())\n",
    "    weighted_average = weighted_sum / total_weight\n",
    "    \n",
    "    return weighted_average\n",
    "\n",
    "metrics_m1 = {\n",
    "    'accuracy': accuracy_m1,\n",
    "    'precision': precision_m1,\n",
    "    'recall': recall_m1,\n",
    "    'specificity': specificity_m1,\n",
    "    'f_score': f_score_m1\n",
    "}\n",
    "\n",
    "metrics_m2 = {\n",
    "    'accuracy': accuracy_m2,\n",
    "    'precision': precision_m2,\n",
    "    'recall': recall_m2,\n",
    "    'specificity': specificity_m2,\n",
    "    'f_score': f_score_m2\n",
    "}\n",
    "\n",
    "weighted_average_m1 = calculate_weighted_average_metrics(metrics_m1)\n",
    "weighted_average_m2 = calculate_weighted_average_metrics(metrics_m2)\n",
    "\n",
    "# Comparar los modelos basados en las métricas ponderadas\n",
    "if weighted_average_m1 > weighted_average_m2:\n",
    "    print(\"Modelo 1 es mejor en términos de métricas ponderadas.\")\n",
    "elif weighted_average_m2 > weighted_average_m1:\n",
    "    print(\"Modelo 2 es mejor en términos de métricas ponderadas.\")\n",
    "else:\n",
    "    print(\"Ambos modelos son comparables en términos de métricas ponderadas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Dada la siguiente matriz de confusión para un problema multiclase, se pide hallar todas las métricas de clasificación para cada clase, excepto el AUC. Recuerde que, para calcular las métricas de una clase, se considera dicha clase como positiva y el resto como la negativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Gato', 'Perro', 'Loro']\n",
    "\n",
    "# Columnas Clase predicha Y\n",
    "# Filas Clase real X\n",
    "confusion_matrix = np.array([ \n",
    "                     [20,10,5], \n",
    "                     [5,30,0], \n",
    "                     [5,5,25] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase: Gato\n",
      "Accuracy: 0.7619047619047619\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 0.5714285714285714\n",
      "Specificity: 0.8571428571428571\n",
      "F-Score: 0.6153846153846153\n",
      "\n",
      "Clase: Perro\n",
      "Accuracy: 0.8095238095238095\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 0.8571428571428571\n",
      "Specificity: 0.7857142857142857\n",
      "F-Score: 0.75\n",
      "\n",
      "Clase: Loro\n",
      "Accuracy: 0.8571428571428571\n",
      "Precision: 0.8333333333333334\n",
      "Recall: 0.7142857142857143\n",
      "Specificity: 0.9285714285714286\n",
      "F-Score: 0.7692307692307692\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cálculo de métricas para cada clase\n",
    "for class_idx, class_name in enumerate(classes):\n",
    "    true_positive = confusion_matrix[class_idx, class_idx]\n",
    "    false_positive = np.sum(confusion_matrix[:, class_idx]) - true_positive\n",
    "    false_negative = np.sum(confusion_matrix[class_idx, :]) - true_positive\n",
    "    true_negative = np.sum(confusion_matrix) - true_positive - false_positive - false_negative\n",
    "\n",
    "    accuracy = (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative)\n",
    "    precision = true_positive / (true_positive + false_positive)\n",
    "    recall = true_positive / (true_positive + false_negative)\n",
    "    specificity = true_negative / (true_negative + false_positive)\n",
    "    f_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    print(f\"Clase: {class_name}\")\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"Specificity:\", specificity)\n",
    "    print(\"F-Score:\", f_score)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
